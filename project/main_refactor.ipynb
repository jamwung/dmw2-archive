{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921a33a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:38.211464Z",
     "start_time": "2023-03-11T14:39:38.182635Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "# HTML('''<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js \"></script><script>\n",
    "# code_show=true; \n",
    "# function code_toggle() {\n",
    "# if (code_show){\n",
    "# $('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
    "# } else {\n",
    "# $('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
    "# }\n",
    "# code_show = !code_show\n",
    "# } \n",
    "# $( document ).ready(code_toggle);</script><form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdad992",
   "metadata": {},
   "source": [
    "# Importation of Libraries & Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7bfdc",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e728d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.471195Z",
     "start_time": "2023-03-11T14:39:38.217831Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom\n",
    "# from assistant import *\n",
    "\n",
    "# Misc\n",
    "import fim\n",
    "import sqlite3\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Clustering\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import (KMeans,\n",
    "                             AgglomerativeClustering,\n",
    "                             DBSCAN, OPTICS, cluster_optics_dbscan)\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "from scipy.spatial.distance import euclidean, cityblock, cosine\n",
    "from sklearn.metrics import (calinski_harabasz_score, davies_bouldin_score,\n",
    "                             silhouette_score, adjusted_mutual_info_score,\n",
    "                             adjusted_rand_score, confusion_matrix)\n",
    "\n",
    "# sns.set_theme('notebook', 'darkgrid', 'colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fec39c",
   "metadata": {},
   "source": [
    "## Global Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015fe88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.488640Z",
     "start_time": "2023-03-11T14:39:41.475603Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_pkl(obj, name, prompt=True):\n",
    "    \"\"\"Save an object to a pickle file.\n",
    "    \"\"\"\n",
    "    folder = 'pickles'\n",
    "    ext = '.pkl'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    if name[-4:] == ext:\n",
    "        fp = os.path.join(folder, name)\n",
    "    else:\n",
    "        fp = os.path.join(folder, name+ext)\n",
    "    joblib.dump(obj, fp)\n",
    "    \n",
    "    if prompt:\n",
    "        print('Object pickled for future use.')\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_pkl(name, prompt=False):\n",
    "    \"\"\"Load an object from a pickle file.\n",
    "    \"\"\"\n",
    "    folder = 'pickles'\n",
    "    ext = '.pkl'\n",
    "    if not os.path.exists(folder):\n",
    "        raise ValueError(\"'pickles' folder does not exist.\")\n",
    "    \n",
    "    if name[-4:] == ext:\n",
    "        fp = os.path.join(folder, name)\n",
    "    else:\n",
    "        fp = os.path.join(folder, name+ext)\n",
    "    pkl = joblib.load(fp)\n",
    "    \n",
    "    if prompt:\n",
    "        print('Pickle file loaded.')\n",
    "    \n",
    "    return pkl\n",
    "\n",
    "\n",
    "def lemmatize(series, stop_words, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"Return a pandas Series of the lemmatized review text data.\n",
    "    \"\"\"\n",
    "    def clean_text(text, stop_words, lemmatizer):\n",
    "        \"\"\"Preprocess the text using lemmatization.\"\"\"\n",
    "        text = text.casefold()\n",
    "        text_list = [\n",
    "            lemmatizer.lemmatize(word)\n",
    "            for word in re.findall(r'\\b[a-z-]+\\b', text)\n",
    "            if word not in stop_words\n",
    "        ]\n",
    "        return ' '.join(text_list)\n",
    "\n",
    "    lemmd_text = (\n",
    "        series.apply(lambda x: clean_text(x, stop_words, lemmatizer))\n",
    "    )\n",
    "    \n",
    "    return lemmd_text\n",
    "\n",
    "def vectorize(corpus, params):\n",
    "    \"\"\"Vectorize the corpus of reviews using `model`.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    sparse_corpus = vectorizer.fit_transform(corpus)\n",
    "    corpus_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        sparse_corpus, columns=vectorizer.get_feature_names_out()\n",
    "    )\n",
    "    \n",
    "    return corpus_df\n",
    "\n",
    "def get_products():\n",
    "    \"\"\"Lemmatize and vectorize product titles for clustering\n",
    "    \"\"\"\n",
    "    try:\n",
    "        product_titles = load_pkl('product_titles')\n",
    "    except:\n",
    "        product_ids = load_pkl('product_ids')\n",
    "        conn = sqlite3.connect(\n",
    "            '/mnt/processed/private/msds2023/lt5/dmw2-project/amazon.db'\n",
    "        )\n",
    "        q = f\"\"\"\n",
    "            SELECT product_id, product_title\n",
    "            FROM products\n",
    "            WHERE product_id in {tuple(product_ids)}\n",
    "        \"\"\"\n",
    "        product_titles = (\n",
    "            pd.read_sql(q, conn).set_index('product_id').squeeze()\n",
    "        )\n",
    "        save_pkl(product_titles, 'product_titles')\n",
    "        \n",
    "    return product_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7b592",
   "metadata": {},
   "source": [
    "## Procedural Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907279cc",
   "metadata": {},
   "source": [
    "1. `db`  : Database creation\n",
    "1. `pr`  : Pruning customers and products\n",
    "1. `vc`  : Creation of TFIDF matrix\n",
    "1. `cl`  : Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39d90a",
   "metadata": {},
   "source": [
    "### Database Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57baf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.509086Z",
     "start_time": "2023-03-11T14:39:41.491083Z"
    }
   },
   "outputs": [],
   "source": [
    "def db_get_files():\n",
    "    \"\"\"Gets the filepaths for the chosen categories of amazon data\n",
    "    \"\"\"\n",
    "    amazon_dir = '/mnt/data/public/amazon-reviews/'\n",
    "\n",
    "    # Get filepaths\n",
    "    cat_paths = {}\n",
    "    cat_list = ['Home', 'Home_Improvement', 'Furniture', 'Major_Appliances']\n",
    "    \n",
    "    for cat in os.listdir(amazon_dir):\n",
    "        match = re.search(r'us_(.*)_v1', cat)\n",
    "        if match is None:\n",
    "            continue\n",
    "        if match.group(1) in cat_list:\n",
    "            cat_paths.update({\n",
    "                match.group(1):\n",
    "                urljoin(amazon_dir, cat)\n",
    "            })\n",
    "    return cat_paths\n",
    "\n",
    "\n",
    "def db_create_products(cat_paths, conn):\n",
    "    \"\"\"Creates database of products\n",
    "    \"\"\"\n",
    "    # Get products\n",
    "    prod_list = {}\n",
    "    for cat, path in cat_paths.items():\n",
    "        prod_list.update(\n",
    "            {cat:\n",
    "            pd.read_csv(\n",
    "                path,\n",
    "                sep='\\t',\n",
    "                compression='gzip',\n",
    "                on_bad_lines='skip',\n",
    "                low_memory=False,\n",
    "                usecols=['product_id', 'product_title', 'product_category'],\n",
    "            )}\n",
    "        )\n",
    "    prod_df = pd.concat(list(prod_list.values()))\n",
    "    \n",
    "    # Drop rows w/ missing values and duplicated products\n",
    "    prod_df.dropna(inplace=True)\n",
    "    prod_df.drop_duplicates(subset=['product_id', 'product_title'],\n",
    "                            inplace=True)\n",
    "    prod_df.drop_duplicates(subset=['product_id'],\n",
    "                            inplace=True)\n",
    "    products = prod_df['product_id'].unique().tolist()\n",
    "    \n",
    "    prod_df.to_sql('products', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    return products\n",
    "\n",
    "\n",
    "def db_create_rates(cat_paths, conn, products):\n",
    "    \"\"\"Creates database of ratings\n",
    "    \"\"\"\n",
    "    # Get ratings\n",
    "    rate_list = {}\n",
    "    for cat, path in cat_paths.items():\n",
    "        rate_list.update(\n",
    "            {cat:\n",
    "            pd.read_csv(\n",
    "                path,\n",
    "                sep='\\t',\n",
    "                compression='gzip',\n",
    "                on_bad_lines='skip',\n",
    "                low_memory=False,\n",
    "                usecols=['customer_id',\n",
    "                         'product_id',\n",
    "                         'review_id',\n",
    "                         'star_rating'],\n",
    "            )}\n",
    "        )\n",
    "    rate_df = pd.concat(list(rate_list.values()))\n",
    "    rate_df = rate_df[rate_df['product_id'].isin(products)]\n",
    "    rate_df.dropna(inplace=True)\n",
    "    rate_df.drop_duplicates(subset=['product_id', 'review_id'],\n",
    "                            inplace=True)\n",
    "    drop_indices = ['2015-07-03', '2015-06-03', '2015-05-15', '2015-02-15',\n",
    "                    '2014-12-03', '2014-11-17', '2014-09-01', '2014-08-09',\n",
    "                    '2014-03-13', '2014-01-19', '2013-10-30']\n",
    "    rate_df.drop(\n",
    "        index=rate_df[rate_df['star_rating'].isin(drop_indices)].index,\n",
    "        inplace=True\n",
    "    )\n",
    "    rate_df['star_rating'] = (\n",
    "        rate_df['star_rating'].astype('float').astype('int')\n",
    "    )\n",
    "    rates = rate_df['review_id'].unique().tolist()\n",
    "    \n",
    "    rate_df.to_sql('ratings', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    return rates\n",
    " \n",
    "\n",
    "def db_create_reviews(cat_paths, conn, rates):\n",
    "    \"\"\"Creates database of reviews\n",
    "    \"\"\"\n",
    "    # Get review content\n",
    "    rev_list = {}\n",
    "    for cat, path in cat_paths.items():\n",
    "        rev_list.update(\n",
    "            {cat:\n",
    "            pd.read_csv(\n",
    "                path,\n",
    "                sep='\\t',\n",
    "                compression='gzip',\n",
    "                on_bad_lines='skip',\n",
    "                low_memory=False,\n",
    "                usecols=['review_id',\n",
    "                         'review_headline',\n",
    "                         'review_body',\n",
    "                         'review_date']\n",
    "            )}\n",
    "        )\n",
    "    rev_df = pd.concat(list(rev_list.values()))\n",
    "    rev_df = rev_df[rev_df['review_id'].isin(rates)]\n",
    "    rev_df.to_sql('reviews', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    \n",
    "def db_create(cat_paths):\n",
    "    \"\"\"Creates a database based on the given categories and filepaths\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(\n",
    "        '/mnt/processed/private/msds2023/lt5/dmw2-project/amazon.db'\n",
    "    )\n",
    "    products = db_create_products(cat_paths, conn)\n",
    "    rates = db_create_rates(cat_paths, conn, products)\n",
    "#     db_create_reviews(cat_paths, conn, rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9d494",
   "metadata": {},
   "source": [
    "### Pruning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92083ebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.519809Z",
     "start_time": "2023-03-11T14:39:41.511276Z"
    }
   },
   "outputs": [],
   "source": [
    "def pr_prune_data(cust_thresh=62, prod_thresh=610):\n",
    "    \"\"\"Prune the number of customers and products to consider based on\n",
    "    thresholds. Return a dataframe of reviews corresponding to considered\n",
    "    customers and products only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_pruned = load_pkl(f'df_pruned_{cust_thresh}_{prod_thresh}')\n",
    "    except:\n",
    "        product_ids = load_pkl('product_ids')\n",
    "        conn = sqlite3.connect(\n",
    "            '/mnt/processed/private/msds2023/lt5/dmw2-project/amazon.db'\n",
    "        )\n",
    "        query = f'''\n",
    "            SELECT customer_id, product_id\n",
    "            FROM ratings\n",
    "            WHERE product_id in {tuple(product_ids)}\n",
    "        '''\n",
    "        ratings_df = pd.read_sql(query, conn)\n",
    "\n",
    "        cust_count = ratings_df.customer_id.value_counts()\n",
    "        prod_count = ratings_df.product_id.value_counts()\n",
    "\n",
    "        considered_customers = (\n",
    "            cust_count[cust_count > cust_thresh].index.tolist()\n",
    "        )\n",
    "        considered_products = (\n",
    "            prod_count[prod_count > prod_thresh].index.tolist()\n",
    "        )\n",
    "\n",
    "        query = f'''\n",
    "            SELECT\n",
    "                ratings.customer_id,\n",
    "                ratings.product_id,\n",
    "                ratings.star_rating\n",
    "            FROM ratings\n",
    "            WHERE ratings.customer_id IN {tuple(considered_customers)}\n",
    "        '''\n",
    "        df_pruned = pd.read_sql(query, conn)\n",
    "        save_pkl(df_pruned, f'df_pruned_{cust_thresh}_{prod_thresh}')\n",
    "        \n",
    "    return df_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac8a8b",
   "metadata": {},
   "source": [
    "### Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60bf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.544040Z",
     "start_time": "2023-03-11T14:39:41.522277Z"
    }
   },
   "outputs": [],
   "source": [
    "def cl_get_features(product_titles, params):\n",
    "    \"\"\"Lemmatize `product_titles` and vectorize for clustering\n",
    "    \"\"\"\n",
    "    # Lemmatize\n",
    "    try:\n",
    "        lemmd_titles = load_pkl('lemmd_titles')\n",
    "    except:\n",
    "        lemmd_titles = lemmatize(product_titles, params['stop_words'])\n",
    "        save_pkl(lemmd_titles, 'lemmd_titles')\n",
    "    \n",
    "    # Vectorize\n",
    "    try:\n",
    "        product_profiles = load_pkl('product_profiles')\n",
    "    except:\n",
    "        product_profiles = vectorize(lemmd_titles, params)\n",
    "        product_profiles.index = product_titles.index\n",
    "        save_pkl(product_profiles, 'product_profiles')\n",
    "        \n",
    "    return product_profiles\n",
    "            \n",
    "\n",
    "def cl_cluster_range(tfidf_matrix, drop_list, n_clusters=9):\n",
    "    \"\"\"Cluster the input matrix using KMeans\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cluster_dict = load_pkl('cluster_dict')\n",
    "    except:\n",
    "        tfidf_matrix = tfidf_matrix.drop(drop_list, axis=1)\n",
    "        cluster_dict = {}\n",
    "        for k in tqdm(range(2, n_clusters+2)):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=1337)\n",
    "            cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "            cluster_dict.update(\n",
    "                {k: cluster_labels}\n",
    "            )\n",
    "        save_pkl(cluster_dict, 'cluster_dict')\n",
    "    \n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "def cl_plot_all(cluster_dict, product_profiles, wc):\n",
    "    \"\"\"Plot a word cloud of the clusters.\n",
    "    \"\"\"\n",
    "    for n_clusters, labels in cluster_dict.items():\n",
    "        n_layers = (n_clusters // 5) + 1\n",
    "        remainder = n_clusters % 5\n",
    "        fig, ax = plt.subplots(n_layers, 5, figsize=(15, n_layers*3))\n",
    "        ax = ax.flatten()\n",
    "        \n",
    "        ax[0].set_title(f'No of Clusters: {n_clusters}',\n",
    "                        loc='left',\n",
    "                        fontsize=16)\n",
    "        for i in range(n_clusters):\n",
    "            idx = np.argwhere(\n",
    "                cluster_dict[n_clusters] == i\n",
    "            ).flatten()\n",
    "            filtered_weights = product_profiles.iloc[idx].sum()\n",
    "            ax[i].imshow(wc.generate_from_frequencies(filtered_weights),\n",
    "                         interpolation='spline16')\n",
    "            ax[i].set_axis_off()\n",
    "            \n",
    "        for j in range(1, 6 - remainder):\n",
    "            fig.delaxes(ax[-j])\n",
    "\n",
    "\n",
    "def cl_plot_cluster(cluster_dict, profiles, n_clusters, wc,\n",
    "                    cluster_names=None):\n",
    "    \"\"\"Plot a word cloud of the cluster.\"\"\"\n",
    "    fig, ax = plt.subplots(n_clusters, 2, figsize=(6.4*2, 4.8*n_clusters),\n",
    "                           dpi=100)\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    ax = ax.flatten()\n",
    "    for i, label in enumerate(np.unique(cluster_dict[n_clusters])):\n",
    "        idx = np.argwhere(\n",
    "            cluster_dict[n_clusters] == label\n",
    "        ).flatten()\n",
    "        filtered_weights = profiles.iloc[idx].sum()\n",
    "        ax[2*i].imshow(wc.generate_from_frequencies(filtered_weights),\n",
    "                       interpolation='spline16')\n",
    "        if cluster_names is not None:\n",
    "            ax[2*i].set_title(cluster_names[i])\n",
    "        ax[2*i].set_axis_off()\n",
    "\n",
    "        top_words = filtered_weights.sort_values(ascending=False).head(14)\n",
    "\n",
    "        sns.barplot(x=top_words.to_numpy(), y=top_words.index, ax=ax[2*i+1],\n",
    "                    color='gold')\n",
    "        \n",
    "        \n",
    "def cl_plot_feat(tfidf_matrix, cluster_dict, num_feats=15, return_fig=False):\n",
    "    \"\"\"Project the features of the TF-IDF matrix onto the SVD components.\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=tfidf_matrix.shape[1],\n",
    "                   random_state=1337)\n",
    "    transformed_profiles = pd.DataFrame(\n",
    "        svd.fit_transform(tfidf_matrix),\n",
    "        index=tfidf_matrix.index\n",
    "    )\n",
    "    sv_idx = np.argsort(svd.explained_variance_ratio_)[::-1]\n",
    "    transformed_profiles = transformed_profiles.iloc[:, sv_idx]\n",
    "    \n",
    "    p = svd.components_.T\n",
    "    features = tfidf_matrix.columns\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6.4*2, 4.8))\n",
    "\n",
    "    sns.scatterplot(ax=ax[0], x=transformed_profiles.iloc[:, 0],\n",
    "                    y=transformed_profiles.iloc[:, 1])\n",
    "    ax[0].set(xlabel='SV1', ylabel='SV2')\n",
    "\n",
    "    top_feats = np.argsort(tfidf_matrix.sum().to_numpy())[::-1][:num_feats]\n",
    "\n",
    "    for feature, vec in zip(features[top_feats], p[top_feats]):\n",
    "        ax[1].arrow(0, 0, vec[0], vec[1], lw=6, ec='none', fc='r')\n",
    "        ax[1].text(vec[0], vec[1], feature, ha='center', color='r',\n",
    "                   fontsize=12)\n",
    "    \n",
    "    if return_fig:\n",
    "        return fig\n",
    "    else:\n",
    "        return transformed_profiles\n",
    "    \n",
    "    \n",
    "def cl_plot_wc(cluster_dict, profiles, n_clusters, label, wc,\n",
    "               cluster_name=None):\n",
    "    \"\"\"Plot a word cloud of the cluster.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(6.4*2, 4.8*2))\n",
    "    idx = np.argwhere(\n",
    "        cluster_dict[n_clusters]['labels'] == label\n",
    "    ).flatten()\n",
    "    filtered_weights = profiles.iloc[idx].sum()\n",
    "    plt.imshow(wc.generate_from_frequencies(filtered_weights),\n",
    "                   interpolation='spline16')\n",
    "    if cluster_name:\n",
    "        plt.title(cluster_name)\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7b092",
   "metadata": {},
   "source": [
    "## Recommender System Functions\n",
    "\n",
    "1. `fim` : Frequent-Itemset Mining (FIM-based) recommender system\n",
    "1. `cb`  : Content-based recommender system\n",
    "1. `pop`  : Popularity-based recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b8bc3",
   "metadata": {},
   "source": [
    "###  FIM-based Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31f465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.611721Z",
     "start_time": "2023-03-11T14:39:41.576238Z"
    }
   },
   "outputs": [],
   "source": [
    "def fim_get_amazon():\n",
    "    \"\"\"Performs clustering on amazon-reviews data using FIM to establish item \n",
    "    profiles.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fim_df_raw = load_pkl('fim_df_raw')\n",
    "        print('Pickle file loaded!')\n",
    "    except:\n",
    "        # Get data needed for transactions\n",
    "        conn = sqlite3.connect(\n",
    "            '/mnt/processed/private/msds2023/lt5/dmw2-project/amazon.db'\n",
    "        )\n",
    "        product_ids = load_pkl('product_ids')\n",
    "        q = f\"\"\"\n",
    "            select \n",
    "                rate.review_id\n",
    "                ,rate.customer_id\n",
    "                ,rate.product_id\n",
    "                ,rate.star_rating\n",
    "                ,rev.review_date\n",
    "            from \n",
    "                ratings as rate\n",
    "                ,reviews as rev\n",
    "            where\n",
    "                rate.review_id = rev.review_id\n",
    "                and rate.product_id in {tuple(product_ids)}\n",
    "        \"\"\"\n",
    "        fim_df_raw = pd.read_sql(q, conn, parse_dates=['review_date'])\n",
    "        save_pkl(fim_df_raw, 'fim_df_raw')\n",
    "        \n",
    "    return fim_df_raw\n",
    "\n",
    "\n",
    "def fim_tune_period(df, n_reviews=None):\n",
    "    \"\"\"Plot number of max elements in a transaction & number of transactions \n",
    "    vs period resampling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fim_df_res = load_pkl('fim_df_res')\n",
    "        print('Pickle file loaded!')\n",
    "    except:\n",
    "        periods = ['2W', '1M', '3M', '6M', 'Y']\n",
    "        max_lens = []\n",
    "        n_transactions = []\n",
    "        single_itemsets = []\n",
    "        for period in tqdm(periods):\n",
    "            df_try = df.copy().iloc[:n_reviews]\n",
    "\n",
    "            df_try['review_date'] = (\n",
    "                df_try.loc[:, 'review_date'].dt.to_period(period)\n",
    "            )\n",
    "            df_try = (\n",
    "                df_try.pivot_table(\n",
    "                    index=['customer_id', 'review_date'],\n",
    "                    columns='product_id',\n",
    "                    values='star_rating',\n",
    "                )\n",
    "            )\n",
    "            means = df_try.mean(axis=1)\n",
    "            df_db = (df_try\n",
    "                     .sub(means, axis=0)\n",
    "                     .reset_index()\n",
    "                     .melt(id_vars=['customer_id', 'review_date'],\n",
    "                           value_name='star_rating'))\n",
    "            mask = df_db['star_rating'] >= 0\n",
    "            df_db = df_db[mask]\n",
    "            df_db = (\n",
    "                df_db.groupby(['customer_id', 'review_date'])\n",
    "                ['product_id'].unique()\n",
    "            )\n",
    "\n",
    "            transaction_lens = df_db.apply(lambda x: len(x))\n",
    "\n",
    "            single_itemsets.append((transaction_lens == 1).sum())\n",
    "            max_lens.append(transaction_lens.max())\n",
    "            n_transactions.append(df_db.shape[0])\n",
    "\n",
    "        fim_df_res = pd.DataFrame(\n",
    "            {'max_lens': max_lens,\n",
    "             'n_transactions': n_transactions,\n",
    "             'single_itemsets': single_itemsets,},\n",
    "            index=periods\n",
    "        )\n",
    "        save_pkl(fim_df_res, 'fim_df_res')\n",
    "        \n",
    "    return fim_df_res\n",
    "\n",
    "\n",
    "def fim_plot_tuning(df_res):\n",
    "    \"\"\"Plot the effects of period length to the different parameters of \n",
    "    transactions\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(df_res.shape[1], 1,\n",
    "                            figsize=(15, 4*df_res.shape[1]))\n",
    "    fig.tight_layout(h_pad=5)\n",
    "    titles = ['Maximum Number of Products in a Transaction',\n",
    "              'Number of Transactions',\n",
    "              'Number of Single-Itemset Transactions']\n",
    "    for i, col in enumerate(df_res.columns):\n",
    "        sns.lineplot(y=df_res[col], x=df_res.index, ax=axs[i])\n",
    "        axs[i].set_title(titles[i], fontsize=15)\n",
    "        \n",
    "\n",
    "def fim_get_rules(df, supp, conf):\n",
    "    \"\"\"Get rules for the recommender system\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fim_df = load_pkl(f'fim_df_{supp}_{conf}')\n",
    "    except:\n",
    "        for _ in trange(1):\n",
    "            try:\n",
    "                df_db = load_pkl('df_db')\n",
    "                print('Transaction database loaded')\n",
    "            except:\n",
    "                print('Transaction database loading failed')\n",
    "                df_db = (\n",
    "                    df\n",
    "                    .groupby(['customer_id'])['product_id']\n",
    "                    .unique()\n",
    "                    .apply(lambda x: list(x))\n",
    "                    .tolist()\n",
    "                )\n",
    "                df_db = [x for x in df_db if len(x) > 1]\n",
    "                save_pkl(df_db, 'df_db')\n",
    "        for _ in trange(1):\n",
    "            try:\n",
    "                num = load_pkl('num')\n",
    "                denom = load_pkl('denom')\n",
    "            except:\n",
    "                num = fim.fpgrowth(\n",
    "                    df_db,\n",
    "                    target='s',\n",
    "                    zmin=2,\n",
    "                    zmax=2,\n",
    "                    supp=supp,\n",
    "                    report='a'\n",
    "                )\n",
    "                save_pkl(num, 'num')\n",
    "                denom = fim.fpgrowth(\n",
    "                    df_db,\n",
    "                    target='s',\n",
    "                    zmin=1,\n",
    "                    zmax=1,\n",
    "                    supp=supp,\n",
    "                    report='a'\n",
    "                )\n",
    "                save_pkl(denom, 'denom')\n",
    "\n",
    "            dict_denom = {x[0][0]: x[1] for x in denom}\n",
    "            results = []\n",
    "            for itemset, value in num:\n",
    "                if value/dict_denom[itemset[0]] >= conf/100:\n",
    "                    results.append({\n",
    "                        'antecedent': itemset[0],\n",
    "                        'consequent': itemset[1],\n",
    "                        'confidence': value/dict_denom[itemset[0]]\n",
    "                    })\n",
    "                elif value/dict_denom[itemset[1]] >= conf/100:\n",
    "                    results.append({\n",
    "                        'antecedent': itemset[1],\n",
    "                        'consequent': itemset[0],\n",
    "                        'confidence': value/dict_denom[itemset[1]]\n",
    "                    })\n",
    "\n",
    "        fim_df = (pd.DataFrame()\n",
    "                  .from_dict(results)\n",
    "                  .pivot_table(index='antecedent',\n",
    "                               columns='consequent',\n",
    "                               values='confidence',\n",
    "                               fill_value=0))\n",
    "        save_pkl(fim_df, f'fim_df_{supp}_{conf}')\n",
    "\n",
    "    return fim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ed780",
   "metadata": {},
   "source": [
    "### Content-based Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d7b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.622507Z",
     "start_time": "2023-03-11T14:39:41.613890Z"
    }
   },
   "outputs": [],
   "source": [
    "def cb_get_user_profiles(df_user_rates, product_profiles, cust_thresh):\n",
    "    \"\"\"Get the user profiles of all users with n_rates above treshold\n",
    "    \"\"\"\n",
    "    try:\n",
    "#         user_profiles = load_pkl('jam_customer_profiles')\n",
    "        user_profiles = load_pkl('user_profiles')\n",
    "    except:\n",
    "        mask = df_user_rates['n_rate'] > cust_thresh\n",
    "        indices = df_user_rates[mask].index\n",
    "\n",
    "        user_profiles = pd.DataFrame(index=indices,\n",
    "                                     columns=product_profiles.columns)\n",
    "\n",
    "        for cust_id in indices:\n",
    "            prod_list = df_user_rates.loc[cust_id, 'product_id']\n",
    "\n",
    "            user_profiles.loc[cust_id, :] = (\n",
    "                product_profiles.loc[prod_list].mean(axis=0)\n",
    "            )\n",
    "\n",
    "        save_pkl(user_profiles, 'user_profiles')\n",
    "        \n",
    "    return user_profiles\n",
    "\n",
    "\n",
    "def cb_get_util(df_prune):\n",
    "    \"\"\"Get utility matrix for content-based algorithms\n",
    "    \"\"\"\n",
    "    df_util = df_prune.pivot_table(\n",
    "        index='customer_id',\n",
    "        columns='product_id',\n",
    "        values='star_rating',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    return df_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8437",
   "metadata": {},
   "source": [
    "### Popularity-based Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ea678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.632018Z",
     "start_time": "2023-03-11T14:39:41.627585Z"
    }
   },
   "outputs": [],
   "source": [
    "def pop_get_weights(df_all):\n",
    "    \"\"\"Get weighted rates for each product\n",
    "    \"\"\"\n",
    "    df_pivot = (\n",
    "        df_all\n",
    "        .groupby('product_id')['star_rating']\n",
    "        .value_counts('rates')\n",
    "        .rename('rates')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_pivot['wRate'] = df_pivot['star_rating'] * df_pivot['rates']\n",
    "    pop_df = df_pivot.groupby('product_id')['wRate'].sum()\n",
    "\n",
    "    return pop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab50fc",
   "metadata": {},
   "source": [
    "### Rating Checker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c7d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.642033Z",
     "start_time": "2023-03-11T14:39:41.634755Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_rates():\n",
    "    \"\"\"Get the data of users with their list of rated items and number of \n",
    "    rated items\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_user_rates = load_pkl('df_user_rates')\n",
    "    except:\n",
    "        conn = sqlite3.connect(\n",
    "            '/mnt/processed/private/msds2023/lt5/dmw2-project/amazon.db'\n",
    "        )\n",
    "        product_ids = load_pkl('product_ids')\n",
    "        q = f\"\"\"\n",
    "            SELECT \n",
    "                rate.review_id\n",
    "                ,rate.customer_id\n",
    "                ,rate.product_id\n",
    "                ,rate.star_rating\n",
    "                ,rev.review_date\n",
    "            FROM \n",
    "                ratings as rate\n",
    "                ,reviews as rev\n",
    "            WHERE\n",
    "                rate.review_id = rev.review_id\n",
    "                AND rate.product_id in {tuple(product_ids)} \n",
    "        \"\"\"\n",
    "        df = pd.read_sql(q, conn, parse_dates=['review_date'])\n",
    "        df_user_rates = (\n",
    "            df\n",
    "            .groupby(['customer_id'])['product_id']\n",
    "            .unique()\n",
    "            .apply(lambda x: list(x))\n",
    "            .to_frame()\n",
    "        )\n",
    "        df_user_rates['n_rate'] = df_user_rates['product_id'].apply(len)\n",
    "        save_pkl(df_user_rates, 'df_user_rates')\n",
    "\n",
    "    return df_user_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f606e",
   "metadata": {},
   "source": [
    "### RecSys360 Class Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d447b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:41:39.347887Z",
     "start_time": "2023-03-11T14:41:39.313145Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecSys360():\n",
    "    def __init__(self,\n",
    "                 fim_df_raw,\n",
    "                 product_profiles,\n",
    "                 prod_clusters,\n",
    "                 cust_thresh=62,\n",
    "                 prod_thresh=610,\n",
    "                 supp=-5,\n",
    "                 conf=60):\n",
    "        \"\"\"Initialize Recommender System\n",
    "        \"\"\"\n",
    "        self.cust_thresh = cust_thresh\n",
    "        self.prod_thresh = prod_thresh\n",
    "        self.supp = supp\n",
    "        self.conf = conf\n",
    "        self.df_prune = pr_prune_data()\n",
    "        self.df_util = cb_get_util(self.df_prune)\n",
    "        self.product_profiles = product_profiles\n",
    "        self.df_user_rates = get_user_rates()\n",
    "        self.user_profiles = cb_get_user_profiles(self.df_user_rates,\n",
    "                                                  product_profiles,\n",
    "                                                  cust_thresh=cust_thresh)\n",
    "        \n",
    "        self.fim_df = fim_get_rules(fim_df_raw, supp, conf)\n",
    "        self.pop_df = pop_get_weights(pr_prune_data(cust_thresh=0,\n",
    "                                                    prod_thresh=0))\n",
    "        self.df_gen = pd.DataFrame(prod_clusters,\n",
    "                                   columns=['labels'],\n",
    "                                   index=product_profiles.index)\n",
    "        \n",
    "                \n",
    "    def fim_recommend(self, user, L):\n",
    "        \"\"\"Recommend items based on established FIM rules based on the liked \n",
    "        value of a user\n",
    "        \"\"\"\n",
    "        likes = self.df_user_rates.loc[user, 'product_id']\n",
    "        w_rules = [item for item in likes if item in self.fim_df.index]\n",
    "        df_reco = (\n",
    "            self.fim_df\n",
    "            .loc[w_rules]\n",
    "            .melt()\n",
    "            .set_index('consequent')\n",
    "            .sort_values('value', ascending=False)\n",
    "        )\n",
    "        mask = df_reco['value'] > 0\n",
    "        recos = df_reco[mask].index.to_list()[:L]\n",
    "\n",
    "        return recos\n",
    "\n",
    "        \n",
    "    def cb_recommend(self, user, M):\n",
    "        \"\"\"\n",
    "        Accepts a utility matrix and item profiles then recommends `L` items \n",
    "        to the user based on their user profile using cosine distance as a \n",
    "        measure. Sort them by item ID in case of equal distance.\n",
    "        \"\"\"\n",
    "        unrated_idx = [\n",
    "            idx for idx\n",
    "            in self.df_util.loc[user][self.df_util.loc[user].isna()].index\n",
    "            if idx in product_profiles.index\n",
    "        ]\n",
    "        distances = []\n",
    "        user_profile = self.user_profiles.loc[user]\n",
    "        for prod_id, row in (self.product_profiles\n",
    "                             .loc[unrated_idx].iterrows()):\n",
    "            if row.sum() == 0:\n",
    "                continue\n",
    "            distances.append(\n",
    "                (prod_id,\n",
    "                 cosine(user_profile, row))\n",
    "            )\n",
    "        recommended = sorted(distances, key=lambda x: (x[1], x[0]))[:M]\n",
    "        recos = [id_ for id_, dist in recommended]\n",
    "\n",
    "        return recos\n",
    "    \n",
    "    \n",
    "    def pop_recommend(self, user, M):\n",
    "        \"\"\"Recommend stratified popular items\n",
    "        \"\"\"\n",
    "        n_reco = M // self.df_gen['labels'].nunique()\n",
    "        remainder = M % self.df_gen['labels'].nunique()\n",
    "        recos = []\n",
    "        for i in range(remainder):\n",
    "            indices =  [idx for idx \n",
    "                        in self.df_gen[self.df_gen['labels'] == i].index\n",
    "                        if idx in self.pop_df.index]\n",
    "            recos.extend(\n",
    "                self.pop_df.loc[indices]\n",
    "                .sort_values(ascending=False)[:n_reco+1]\n",
    "                .index\n",
    "                .to_list()\n",
    "            )\n",
    "\n",
    "        for j in range(remainder, self.df_gen['labels'].nunique()):\n",
    "            indices =  [idx for idx \n",
    "                        in self.df_gen[self.df_gen['labels'] == j].index\n",
    "                        if idx in self.pop_df.index]\n",
    "            recos.extend(\n",
    "                self.pop_df[indices]\n",
    "                .sort_values(ascending=False)[:n_reco]\n",
    "                .index\n",
    "                .to_list()\n",
    "            )\n",
    "\n",
    "        return recos[:M]\n",
    "    \n",
    "    def recommend(self, user, L=5, return_titles=True, print_dist=False, print_wc=False):\n",
    "        \"\"\"Implementation of hybrid recommender system.\n",
    "        \"\"\"\n",
    "        recos = self.fim_recommend(user, L=L)\n",
    "        \n",
    "        n_rate = self.df_user_rates.loc[user, 'n_rate']\n",
    "        M = L - len(recos)\n",
    "        if n_rate > self.cust_thresh and M > 0:\n",
    "            recos.extend(self.cb_recommend(user, M))\n",
    "            if print_dist:\n",
    "                print(f'Recommended Items:\\n'\n",
    "                      f'{L-M} FIM-based items, {M} CB-based items')\n",
    "\n",
    "        elif n_rate <= self.cust_thresh and M > 0:\n",
    "            recos.extend(self.pop_recommend(user, M))\n",
    "            if print_dist:\n",
    "                print(f'Recommended Items:\\n'\n",
    "                      f'{L-M} FIM-based items, {M} Popularity-based items')\n",
    "        \n",
    "        elif M <= L and print_dist:\n",
    "            print(f'Recommended Items:\\n'\n",
    "                  f'{L-M} FIM-based items')\n",
    "            \n",
    "        if return_titles:\n",
    "            conn = sqlite3.connect(\n",
    "                '/mnt/processed/private/msds2023/lt5/dmw2-project/amazon.db'\n",
    "            )\n",
    "            query = f'''\n",
    "                SELECT product_title\n",
    "                FROM products\n",
    "                WHERE product_id IN {tuple(recos)}\n",
    "            '''\n",
    "            return pd.read_sql(query, conn).squeeze().tolist()\n",
    "        \n",
    "        if print_wc:\n",
    "            wc = WordCloud(\n",
    "                width=800, height=800,\n",
    "                mask=None,\n",
    "                background_color='darkslategray',\n",
    "                colormap='Wistia',\n",
    "                max_words=400,\n",
    "                collocations = False,\n",
    "                stopwords=stop_words,\n",
    "                random_state=1337,\n",
    "            )\n",
    "            \n",
    "            user_profile = self.user_profiles.loc[user]\n",
    "            with sns.axes_style('white'):\n",
    "                plt.figure(figsize = (15, 4.8*2))\n",
    "                plt.imshow(wc.generate_from_frequencies(user_profile),\n",
    "                           interpolation='spline16')\n",
    "                plt.title('User: {user}')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "        return recos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db86810",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8efe4f",
   "metadata": {},
   "source": [
    "## Data Collection and Database Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd7783",
   "metadata": {},
   "source": [
    "Inserts text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2a6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:41.708678Z",
     "start_time": "2023-03-11T14:39:41.705769Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get chosen categories' filepaths\n",
    "# f_paths = db_get_files()\n",
    "# # Run Database Creation Function\n",
    "# db_create(f_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4565a9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c80ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09d01d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:43.162377Z",
     "start_time": "2023-03-11T14:39:41.711171Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all available Products\n",
    "product_titles = get_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d425351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:44.955618Z",
     "start_time": "2023-03-11T14:39:43.164626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define other drop words\n",
    "drop_words = ['let', 'one', 'two', 'three', 'four', 'five', 'star', 'product',\n",
    "              'year', 'inch', 'warranty', 'x', 'recommend', 'well', 'great',\n",
    "              'size', 'expected', 'best', 'excellent', 'inch', 'pack', 'px',\n",
    "              'pk', 'ct', 'quality', 'recommend', 'item', 'wonderful', 'size',\n",
    "              'must', 'ordered', 'order', 'wanted', 'want', 'super', 'star',\n",
    "              'purchase', 'useful', 'arrived', 'arrive', 'awesome', 'super',\n",
    "              'fantastic', 'quite', 'definitely', 'worked', 'pleased', 'sure',\n",
    "              'know', 'every', 'second', 'purchased', 'lb', 'pound', 'inch',\n",
    "              'cleaner', 'ounce', 'oz', 'pack', 'product', 'quality']\n",
    "\n",
    "drop_list = ['white', 'black', 'blue', 'red', 'home', 'set', 'piece', 'color']\n",
    "\n",
    "# Instatiate stop words\n",
    "stop_words = list(set(\n",
    "    stopwords.words('english') +\n",
    "    list(STOPWORDS) +\n",
    "    drop_words\n",
    "))\n",
    "\n",
    "# Define params for tf-idf vectorization\n",
    "new_params = dict(\n",
    "    ngram_range=(1, 1),\n",
    "    token_pattern=r'[a-z]{2,}',\n",
    "    stop_words=stop_words,\n",
    "    max_df=0.10,\n",
    "    min_df=0.005,\n",
    "    max_features=None\n",
    ")\n",
    "\n",
    "# Get product profiles\n",
    "product_profiles = cl_get_features(product_titles, new_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ff565",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6c9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:59.239629Z",
     "start_time": "2023-03-11T14:39:59.178870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get cluster labels for each k in range\n",
    "cluster_dict = cl_cluster_range(product_profiles, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219de92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:44.982630Z",
     "start_time": "2023-03-11T14:39:44.977563Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Plot all wordclouds for each k-clustering\n",
    "# wc = WordCloud(\n",
    "#     font_path='AmazonEmberCdRC_Rg.ttf',\n",
    "#     width=800, height=800,\n",
    "#     background_color='white',\n",
    "#     colormap='CMRmap_r',\n",
    "#     stopwords=stop_words,\n",
    "#     random_state=1337,\n",
    "# )\n",
    "# cl_plot_all(cluster_dict, product_profiles, wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75bd0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:44.992223Z",
     "start_time": "2023-03-11T14:39:44.986315Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Label clusters for n_clusters = 8\n",
    "# n = 8\n",
    "# cluster_names = ['Christmas Decorations',\n",
    "#                  'Wood Furnitures',\n",
    "#                  'Light Fixtures',\n",
    "#                  'Bed Essentials',\n",
    "#                  'Movie Posters',\n",
    "#                  'Wall Decorations', \n",
    "#                  'Bed Features',\n",
    "#                  'Kitchen Fixtures']\n",
    "# cl_plot_cluster(cluster_dict,\n",
    "#                 product_profiles.drop(drop_list, axis=1),\n",
    "#                 n,\n",
    "#                 wc,\n",
    "#                 cluster_names=cluster_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d105f",
   "metadata": {},
   "source": [
    "## FIM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0120844",
   "metadata": {},
   "source": [
    "Try to reduce number of transactions using `customer_id` and resampled `review_date`s as basis for transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66f9d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:48.623544Z",
     "start_time": "2023-03-11T14:39:44.997412Z"
    }
   },
   "outputs": [],
   "source": [
    "fim_df_raw = fim_get_amazon()\n",
    "# fim_df_res = fim_tune_period(fim_df_raw, n_reviews=10_000)\n",
    "# fim_plot_tuning(fim_df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59337a",
   "metadata": {},
   "source": [
    "There is no significant effect in resampling `review_date`s, hence, we can simply treat each unique `customer_id` as a transaction of liked products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de98d20",
   "metadata": {},
   "source": [
    "## Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dad424",
   "metadata": {},
   "source": [
    "Initialize the RecSys360 system with the following necessary databases:\n",
    "\n",
    "1. FIM Algorithm\n",
    "    * Transaction Database\n",
    "    * FIM Rules with hyperparameters: `supp` & `conf`\n",
    "\n",
    "\n",
    "2. Content-based Algorithm\n",
    "    * Item Profiles\n",
    "    * User Profiles\n",
    "\n",
    "\n",
    "3. Popularity-based Algorithm\n",
    "    * Weighted product/item ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6db1e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:42:18.570462Z",
     "start_time": "2023-03-11T14:41:47.598155Z"
    }
   },
   "outputs": [],
   "source": [
    "recsys360 = RecSys360(fim_df_raw, product_profiles, cluster_dict[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b24a1",
   "metadata": {},
   "source": [
    "### FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1fcb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:42:19.701567Z",
     "start_time": "2023-03-11T14:42:18.580404Z"
    }
   },
   "outputs": [],
   "source": [
    "recsys360.recommend(52228204, print_dist=True, print_wc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01142893",
   "metadata": {},
   "source": [
    "### FIM-CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db7c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:49.270544Z",
     "start_time": "2023-03-11T14:39:49.270530Z"
    }
   },
   "outputs": [],
   "source": [
    "recsys360.recommend(10804961, print_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7eb47",
   "metadata": {},
   "source": [
    "### FIM-Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b492a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:39:49.274208Z",
     "start_time": "2023-03-11T14:39:49.274193Z"
    }
   },
   "outputs": [],
   "source": [
    "recsys360.recommend(10032, print_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7d487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "541.6px",
    "left": "24px",
    "top": "186.725px",
    "width": "303.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
